# numpy_neuron_network
仅使用numpy从头构建神经网络, 包括如下内容(更新中....)

0. 网络中梯度反向传播公式推导


1. 层：FC层,卷积层,池化层,Flatten
2. 激活函数: ReLU、LeakyReLU、PReLU、ELU、SELU
3. 损失函数：均方差、交叉熵
4. 模型的保存、部署
5. 案例学习：线性回归、图像分类
6. 迁移学习、模型精调
7. 进阶：RNN、LSTM、GRU、BN

[TOC]

## 基础知识

[0_1-全连接层、损失函数的反向传播](0_1-全连接层、损失函数的反向传播.md)

[0_2_1-卷积层的反向传播-单通道、无padding、步长1](0_2_1-卷积层的反向传播-单通道、无padding、步长1.md)

[0_2_2-卷积层的反向传播-多通道、无padding、步长1](0_2_2-卷积层的反向传播-多通道、无padding、步长1.md)

[0_2_3-卷积层的反向传播-多通道、无padding、步长不为1](0_2_3-卷积层的反向传播-多通道、无padding、步长不为1.md)

[0_2_4-卷积层的反向传播-多通道、有padding、步长不为1](0_2_4-卷积层的反向传播-多通道、有padding、步长不为1.md)

[0_2_5-池化层的反向传播-MaxPooling、AveragePooling、GlobalAveragePooling、GlobalMaxPooling](0_2_5-池化层的反向传播-Max Pooling、Average Pooling、Global Average Pooling.md)

[0_3-激活函数的反向传播-ReLU、LeakyReLU、PReLU、ELU、SELU](0_3-激活函数的反向传播-ReLU、LeakyReLU、PReLU、ELU、SELU.md)

[0_4-优化方法-SGD、AdaGrad、RMSProp、Adadelta、Adam](0_4-优化方法-SGD、AdaGrad、RMSProp、Adadelta、Adam.md)





## DNN练习

[1_1_1-全连接神经网络做线性回归](1_1_1-全连接神经网络做线性回归.md)

[1_1_2-全连接神经网络做mnist手写数字识别](1_1_2-全连接神经网络做mnist手写数字识别.md)



## CNN练习

2_1-卷积层实现

2_2-池化层实现

2_3-手写数字识别

2_4-对抗神经网络



## 其它

模型的保存、部署

精调网络





## 进阶

5-1-RNN反向传播

5-2-LSTM反向传播

5-3-GRU反向传播

5-4-RNN、LSTM、GRU实现

5-5-案例-lstm连续文字识别



6-1-Batch Normalization反向传播

6-2-Batch Normalization实现









