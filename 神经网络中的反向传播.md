



## 依赖知识

a) 熟悉神经网络的基础知识

b) 熟悉导数的链式法则及常见函数的导数



## 约定说明

a) 对于一个$n$层神经网络,第$i$层表示为$l_i, \  1 \le i \le n$ ,第$i$神经元个数为$|l_i|$ ; 注意$l_i$是输入层, $l_n$ 是输出层。



b) 对于神经网络的一个输入样本$x$ ,其维度为$(1,|l_1|)$ ,也就是一个行向量；对应的输出$y$ 也是一个行向量，维度为$(1,|l_n|)$ 。(注：也可以将$x$ 和 $y$ 表示为列向量，这里预先说明好为行向量，避免后面有歧义)



c) 设神经网络中第i层的输出为$z^i$,($z^i$ 都是行向量)则 $x=z^1, y=z^n$ ; 第$i$ 层的权重和偏置分别为$W^i, b^i$ ;则对于全连接层$z^{i+1} = z^iW^i + b^i$ ; 其中$W^i$ 和$b^i$的维度分别为为$(|l_i|,|l_{i+1}|),(1,|l_{i+1}|)$ 



d) 定义损失函数为$L(y,y^*)$ ;其中$y*$ 为样本的真实$y$值



## 误差反向

a) 记损失函数L关于第$i$ 层神经元的输出$z^i$ 的偏导为$\delta^i = \frac {\partial L} {\partial z^i}$

b) 首先我们来看看损失函数L在最后一层参数上的偏导;也就是$\frac {\partial L} {\partial W^{n-1}}$ 和 $\frac {\partial L} {\partial b^{n-1}}$
$$
\frac {\partial L} {\partial W^{n-1}_{i,j}} \ \ \ (1) \\
= \frac {\partial L} {z^n_j} * \frac {\partial z^n_j} {\partial W^{n-1}_{i,j}}  \ \ \ (2) \\
=  \delta_j^n * \frac {\partial (z^{n-1} W^{n-1}+b^{n-1})} {\partial W^{n-1}_{i,j}} \ \ \ (3) \\
= \delta_j^n * z^{n-1}_i \ \ \ \ (4)
$$

$$
\frac {\partial L} {\partial W^{n-1}} = (z^{n-1}_i)^T \delta^n
$$

c) 



 $Wx+b$ 

