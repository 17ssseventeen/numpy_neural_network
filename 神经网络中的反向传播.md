



## 依赖知识

a) 熟悉神经网络的基础知识

b) 熟悉导数的链式法则及常见函数的导数

c) 熟悉常见的优化方法，梯度下降，随机梯度下降等



## 约定说明

a) 对于一个$n$层神经网络,第$i$层表示为$l_i, \  1 \le i \le n$ ,第$i$神经元个数为$|l_i|$ ; 注意$l_i$是输入层, $l_n$ 是输出层。



b) 对于神经网络的一个输入样本$x$ ,其维度为$(1,|l_1|)$ ,也就是一个行向量；对应的输出$y$ 也是一个行向量，维度为$(1,|l_n|)$ 。(注：也可以将$x$ 和 $y$ 表示为列向量，这里预先说明好为行向量，避免后面有歧义)



c) 设神经网络中第i层的输出为$z^i$,($z^i$ 都是行向量)则 $x=z^1, y=z^n$ ; 第$i$ 层的权重和偏置分别为$W^i, b^i$ ;则对于全连接层$z^{i+1} = z^iW^i + b^i$ ; 其中$W^i$ 和$b^i$的维度分别为为$(|l_i|,|l_{i+1}|),(1,|l_{i+1}|)$ 



d) 定义损失函数为$L(y,y^*)$ ;其中$y*$ 为样本的真实$y$值



## 误差反向

a) 记损失函数L关于第$i$ 层神经元的输出$z^i$ 的偏导为$\delta^i = \frac {\partial L} {\partial z^i}  \ \ \ (1)$

b) 首先我们来看看损失函数L在最后一层参数上的偏导;也就是$\frac {\partial L} {\partial W^{n-1}}$ 和 $\frac {\partial L} {\partial b^{n-1}}$
$$
\begin{align*}
& \frac {\partial L} {\partial W^{n-1}_{i,j}} \ \ \  \\
& = \frac {\partial L} {z^n_j} * \frac {\partial z^n_j} {\partial W^{n-1}_{i,j}}  \ \ \ (2)  \ \ \ \ //W^{n-1}_{i,j}是第n-1层的第i个神经元和第n层的第j个神经元的连接，所以只有z^n_j的误差经过W^{n-1}_{i,j}反向传播 \\
& =  \delta_j^n * \frac {\partial (\sum_{k=1}^{|l_{n-1}|}z^{n-1}_k W^{n-1}_{k,j}+b^{n-1}_j)} {\partial W^{n-1}_{i,j}} \ \ \  (3) \ \ \  //z^n_j是z^{n-1}这个行向量与权重矩阵W^{n-1}的第j列向量的乘积,加上偏置b^{n-1}_j\\
& = \delta_j^n * z^{n-1}_i \ \ \ \ (4)
\end{align*}
$$



对等式(4)一般化的向量表示为：
$$
\frac {\partial L} {\partial W^{n-1}} = (z^{n-1})^T \delta^n_j  \tag 5
$$

同理可得：
$$
\frac {\partial L} {\partial b^{n-1}} =\delta^n  \tag 6
$$


c) 更一般的损失函数L关于第$l$层(这里没有用索引$i$,避免跟等式1~4中的索引名相同，引起理解障碍)的参数上的偏导，也就是$\frac {\partial L} {\partial W^l}$ 和 $\frac {\partial L} {\partial b^l}$
$$
\frac {\partial L} {\partial W^l} = \frac {\partial L} {\partial z^{l+1}} * \frac {\partial z^{l+1}} {\partial W^l}    \ \ \ \ (7)  \\
= (z^l)^T\delta^{l+1}  \ \ \ \ (8)
$$
同理可得：
$$
\frac {\partial L} {\partial b^l} =\delta^{l+1} \tag 9
$$
d) 现在我们来看a)中定义的损失函数L关于第$l$层输出的偏导$\delta^l = \frac {\partial L} {\partial z^l}$
$$
\begin{align*}
&\delta^l_i=\frac {\partial L} {\partial z^l_i}  \ \ \   \\
&=\frac {\partial L} {\partial z^{l+1}} * \frac {\partial z^{l+1}} {\partial z^l_i}  \ \ \  (10)  \ \ \ \ //导数的链式法则\\
&=\frac {\partial L} {\partial z^{l+1}} * \frac {\partial (z^lW^l + b^l)} {\partial z^l_i} \ \ \  (11)  \ \ \ \ //z^l的定义\\
&=\sum_{j=1}^{|l_{l+1}|} \frac {\partial L} {\partial z^{l+1}_j} *  \frac {\partial (\sum_{k=1}^{|l_l|}z^l_k W^l_{k,j}+b^l_j)} {\partial z^l_i} \ \ \  (12)  \ \ \ \ //第l+1层的每个节点都有梯度专递到l层的第i个节点\\
&=\sum_{j=1}^{|l_{l+1}|} \delta^{l+1}_j * W^l_{i,j} \ \ \  (13)  \ \ \ \ //只有k=i时有梯度\\
&=\delta^{l+1} ((W^l)^T)_i  \ \ \  (14)  \ \ \ \ // 可以表示为l+1层梯度的行向量与权重W^l 的转置的第i个列向量的乘积

\end{align*}
$$
一般化的表示如下：
$$
\delta^l = \frac {\partial L} {\partial z^l} \\
=\delta^{l+1}(W^l)^T \ \ \ (15)
$$

### 结论

​        以上的证明就是为了说明下面的结论，所以请牢记以下的结论，后续自己写神经网络的反向传播都会用到以下结论

a) 根据公式15，损失函数L关于第$l$层神经元的偏导，就是第$l+1$ 层的偏导乘上第l层权重矩阵的转置。我们将公式(15)递归一下,$\delta^l=\delta^{l+1}(W^l)^T=\delta^{l+2}(W^{l+1})^T(W^l)^T=\delta^n(W^{n-1})^T(W^{n-2})^T...(W^l)^T \ \ (16)$

​    也就是说，只要求得损失函数L关于最后一层(n)的偏导$\delta^n$,其它任意层的偏导，直接用最后一层的偏导逐层乘上权重的转置即可。很简单，很对称，有木有？



b) 根据公式(8) ,损失函数L关于第$l$层权重$W^l$ 的偏导为，第$l$ 层输出的转置乘第$l+1$ 层的偏导$\frac {\partial L} {\partial W^l} = (z^l)^T\delta^{l+1}  \ \ \ \ (8)$



c) 根据公式(9) ,损失函数L关于第$l$层偏置$b^l$ 的偏导,就是第$l+1$ 层的偏导: $\frac {\partial L} {\partial b^l} =\delta^{l+1}  \ \ \ (9)$

​         由以上可知对任意的全连接层，我们只需要只到它后一层的偏导，就可以求得当前层参数(权重、偏置)的偏导; 就可以使用梯度下降算法更新参数了 
$$
w = w - \eta * \frac {\partial L} {\partial w}             \ \ //\eta为学习率 \tag {17} 
$$

$$
b = b - \eta * \frac {\partial L} {\partial b}  \tag {18}
$$

​          根据公式(16), 损失函数L关于任何一层的偏导,只需要求导损失函数关于最后一层的偏导$\delta^n$ 即可。$\delta^n$ 与损失函数的定义有关，下一节介绍常用损失函数的偏导计算。