{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、定义前向、后向传播\n",
    "\n",
    "本文将用numpy实现全连接层的前向过程和反向过程，并使用一个线性回归作为例子进行测试；\n",
    "\n",
    "如果对神经网络的反向传播过程还有不清楚的，可以[0_1-全连接层、损失函数的反向传播](0_1-全连接层、损失函数的反向传播.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fc_forword(z, W, b):\n",
    "    \"\"\"\n",
    "    全连接层的前向传播\n",
    "    :param z: 当前层的输出\n",
    "    :param W: 当前层的权重\n",
    "    :param b: 当前层的偏置\n",
    "    :return: 下一层的输出\n",
    "    \"\"\"\n",
    "    return np.dot(z, W) + b\n",
    "\n",
    "\n",
    "def fc_backword(next_dz, W, z):\n",
    "    \"\"\"\n",
    "    全连接层的反向传播\n",
    "    :param next_dz: 下一层的梯度\n",
    "    :param W: 当前层的权重\n",
    "    :param z: 当前层的输出\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dz = np.dot(next_dz, W.T)  # 当前层的梯度\n",
    "    dw = np.dot(z.reshape(1,-1).T, next_dz)  # 当前层权重的梯度\n",
    "    db = next_dz  # 当前层偏置的梯度\n",
    "    return dw, db, dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_squared_loss(y_predict,y_true):\n",
    "    \"\"\"\n",
    "    均方误差损失函数\n",
    "    :param y_predict: 预测值,shape (N,d)，N为批量样本数\n",
    "    :param y_true: 真实值\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    loss = np.mean(np.sum(np.square(y_predict-y_true),axis=-1))  # 损失函数值\n",
    "    dy = y_predict - y_true  # 损失函数关于网络输出的梯度\n",
    "    return loss, dy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、初始化数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:(500, 2),y.shape:(500, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 实际的权重和偏置\n",
    "W = np.array([[3,7,4],\n",
    "              [5,2,6]])\n",
    "b = np.array([2,9,3])\n",
    "\n",
    "# 产生训练样本\n",
    "x_data = np.random.randint(0,10,1000).reshape(500,2)\n",
    "y_data = np.dot(x_data,W)+b\n",
    "\n",
    "def next_sample():\n",
    "    idx=np.random.randint(500)\n",
    "    return x_data[idx],y_data[idx]\n",
    "\n",
    "print(\"x.shape:{},y.shape:{}\".format(x_data.shape,y_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、定义网络、使用SGD训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "迭代1000次，当前loss:1.4122204760270454, 当前权重:[[3.02023707 7.11048368 4.0306883 ]\n",
      " [5.02551551 2.13930116 6.03869273]],当前偏置[[1.69429809 7.33102967 2.53642149]]\n",
      "\n",
      "迭代2000次，当前loss:0.0667536435826483, 当前权重:[[3.00496934 7.02712998 4.00753571]\n",
      " [5.00366117 2.01998803 6.00555194]],当前偏置[[1.93782921 8.66058044 2.90572174]]\n",
      "\n",
      "迭代3000次，当前loss:9.784485689113114e-05, 当前权重:[[3.00092379 7.0050434  4.00140087]\n",
      " [5.00142559 2.00778297 6.00216182]],当前偏置[[1.98774673 8.93310366 2.98141866]]\n",
      "\n",
      "迭代4000次，当前loss:6.480848665983283e-06, 当前权重:[[3.0003127  7.00170716 4.00047419]\n",
      " [5.00019792 2.00108055 6.00030014]],当前偏置[[1.99749505 8.98632432 2.9962014 ]]\n",
      "\n",
      "迭代5000次，当前loss:5.494742446260978e-07, 当前权重:[[3.00003135 7.00017117 4.00004754]\n",
      " [5.00006096 2.00033284 6.00009245]],当前偏置[[1.99952979 8.99743293 2.99928696]]\n",
      "\n",
      "迭代6000次，当前loss:1.0817257575795559e-10, 当前权重:[[3.00000567 7.00003094 4.00000859]\n",
      " [5.00000837 2.0000457  6.00001269]],当前偏置[[1.99990451 8.9994787  2.9998552 ]]\n",
      "\n",
      "迭代7000次，当前loss:8.280144849774796e-10, 当前权重:[[3.00000072 7.00000394 4.00000109]\n",
      " [5.00000183 2.00000999 6.00000278]],当前偏置[[1.99998196 8.99990148 2.99997264]]\n",
      "\n",
      "迭代8000次，当前loss:4.2198209315202926e-12, 当前权重:[[3.00000028 7.00000151 4.00000042]\n",
      " [5.00000056 2.00000303 6.00000084]],当前偏置[[1.99999642 8.99998045 2.99999457]]\n",
      "\n",
      "迭代9000次，当前loss:1.2520595110571036e-11, 当前权重:[[3.00000004 7.0000002  4.00000006]\n",
      " [5.00000007 2.00000036 6.0000001 ]],当前偏置[[1.99999925 8.99999593 2.99999887]]\n",
      "\n",
      "迭代10000次，当前loss:1.0441840844140337e-13, 当前权重:[[3.00000002 7.0000001  4.00000003]\n",
      " [5.00000002 2.00000009 6.00000003]],当前偏置[[1.99999984 8.99999912 2.99999976]]\n",
      "\n",
      "迭代11000次，当前loss:2.3555979316041686e-16, 当前权重:[[3.         7.00000002 4.00000001]\n",
      " [5.         2.00000002 6.        ]],当前偏置[[1.99999997 8.99999983 2.99999995]]\n",
      "\n",
      "迭代11790次，当前loss:3.2760254909770205e-22, 当前权重:[[3. 7. 4.]\n",
      " [5. 2. 6.]],当前偏置[[1.99999999 8.99999995 2.99999999]]\n"
     ]
    }
   ],
   "source": [
    "# 随机初始化参数\n",
    "W1 = np.random.randn(2,3)\n",
    "b1 = np.zeros([1,3])\n",
    "loss = 100.0\n",
    "lr = 0.01\n",
    "i = 0 \n",
    "\n",
    "while loss > 1e-20:\n",
    "    x,y_true=next_sample()  # 获取当前样本\n",
    "    # 前向传播\n",
    "    y = fc_forword(x,W1,b1)\n",
    "    # 反向传播更新梯度\n",
    "    loss,dy=mean_squared_loss(y,y_true)\n",
    "    dw,db,_ = fc_backword(dy,W,x)\n",
    "    \n",
    "    # 在一个batch中梯度取均值\n",
    "    #print(dw)\n",
    "    \n",
    "    # 更新梯度\n",
    "    W1 -= lr*dw\n",
    "    b1 -= lr*db\n",
    "    \n",
    "    # 更新迭代次数\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print(\"\\n迭代{}次，当前loss:{}, 当前权重:{},当前偏置{}\".format(i,loss,W1,b1))   \n",
    "\n",
    "# 打印最终结果\n",
    "print(\"\\n迭代{}次，当前loss:{}, 当前权重:{},当前偏置{}\".format(i,loss,W1,b1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1==W: True \n",
      "b1=b:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"W1==W: {} \\nb1=b:  {}\".format(np.allclose(W1,W),np.allclose(np.squeeze(b1),b)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
