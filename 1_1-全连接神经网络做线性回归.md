[TOC]

## 一、定义前向、后向传播

本文将用numpy实现全连接层的前向过程和反向过程，并使用一个线性回归作为例子进行测试；

如果对神经网络的反向传播过程还有不清楚的，可以[0_1-全连接层、损失函数的反向传播](0_1-全连接层、损失函数的反向传播.md)




```python
import numpy as np

def fc_forword(z, W, b):
    """
    全连接层的前向传播
    :param z: 当前层的输出
    :param W: 当前层的权重
    :param b: 当前层的偏置
    :return: 下一层的输出
    """
    return np.dot(z, W) + b


def fc_backword(next_dz, W, z):
    """
    全连接层的反向传播
    :param next_dz: 下一层的梯度
    :param W: 当前层的权重
    :param z: 当前层的输出
    :return:
    """
    dz = np.dot(next_dz, W.T)  # 当前层的梯度
    dw = np.dot(z.reshape(1,-1).T, next_dz)  # 当前层权重的梯度
    db = next_dz  # 当前层偏置的梯度
    return dw, db, dz
```

## 二、定义损失函数


```python
def mean_squared_loss(y_predict,y_true):
    """
    均方误差损失函数
    :param y_predict: 预测值,shape (N,d)，N为批量样本数
    :param y_true: 真实值
    :return:
    """
    loss = np.mean(np.sum(np.square(y_predict-y_true),axis=-1))  # 损失函数值
    dy = y_predict - y_true  # 损失函数关于网络输出的梯度
    return loss, dy
```

## 三、初始化数据


```python

# 实际的权重和偏置
W = np.array([[3,7,4],
              [5,2,6]])
b = np.array([2,9,3])

# 产生训练样本
x_data = np.random.randint(0,10,1000).reshape(500,2)
y_data = np.dot(x_data,W)+b

def next_sample():
    idx=np.random.randint(500)
    return x_data[idx],y_data[idx]

print("x.shape:{},y.shape:{}".format(x_data.shape,y_data.shape))
```

    x.shape:(500, 2),y.shape:(500, 3)
​    

## 四、定义网络、使用SGD训练


```python
# 随机初始化参数
W1 = np.random.randn(2,3)
b1 = np.zeros([1,3])
loss = 100.0
lr = 0.01
i = 0 

while loss > 1e-20:
    x,y_true=next_sample()  # 获取当前样本
    # 前向传播
    y = fc_forword(x,W1,b1)
    # 反向传播更新梯度
    loss,dy=mean_squared_loss(y,y_true)
    dw,db,_ = fc_backword(dy,W,x)
    
    # 在一个batch中梯度取均值
    #print(dw)
    
    # 更新梯度
    W1 -= lr*dw
    b1 -= lr*db
    
    # 更新迭代次数
    i += 1
    if i % 1000 == 0:
        print("\n迭代{}次，当前loss:{}, 当前权重:{},当前偏置{}".format(i,loss,W1,b1))   

# 打印最终结果
print("\n迭代{}次，当前loss:{}, 当前权重:{},当前偏置{}".format(i,loss,W1,b1))  
```


    迭代1000次，当前loss:1.4122204760270454, 当前权重:[[3.02023707 7.11048368 4.0306883 ]
     [5.02551551 2.13930116 6.03869273]],当前偏置[[1.69429809 7.33102967 2.53642149]]
    
    迭代2000次，当前loss:0.0667536435826483, 当前权重:[[3.00496934 7.02712998 4.00753571]
     [5.00366117 2.01998803 6.00555194]],当前偏置[[1.93782921 8.66058044 2.90572174]]
    
    迭代3000次，当前loss:9.784485689113114e-05, 当前权重:[[3.00092379 7.0050434  4.00140087]
     [5.00142559 2.00778297 6.00216182]],当前偏置[[1.98774673 8.93310366 2.98141866]]
    
    迭代4000次，当前loss:6.480848665983283e-06, 当前权重:[[3.0003127  7.00170716 4.00047419]
     [5.00019792 2.00108055 6.00030014]],当前偏置[[1.99749505 8.98632432 2.9962014 ]]
    
    迭代5000次，当前loss:5.494742446260978e-07, 当前权重:[[3.00003135 7.00017117 4.00004754]
     [5.00006096 2.00033284 6.00009245]],当前偏置[[1.99952979 8.99743293 2.99928696]]
    
    迭代6000次，当前loss:1.0817257575795559e-10, 当前权重:[[3.00000567 7.00003094 4.00000859]
     [5.00000837 2.0000457  6.00001269]],当前偏置[[1.99990451 8.9994787  2.9998552 ]]
    
    迭代7000次，当前loss:8.280144849774796e-10, 当前权重:[[3.00000072 7.00000394 4.00000109]
     [5.00000183 2.00000999 6.00000278]],当前偏置[[1.99998196 8.99990148 2.99997264]]
    
    迭代8000次，当前loss:4.2198209315202926e-12, 当前权重:[[3.00000028 7.00000151 4.00000042]
     [5.00000056 2.00000303 6.00000084]],当前偏置[[1.99999642 8.99998045 2.99999457]]
    
    迭代9000次，当前loss:1.2520595110571036e-11, 当前权重:[[3.00000004 7.0000002  4.00000006]
     [5.00000007 2.00000036 6.0000001 ]],当前偏置[[1.99999925 8.99999593 2.99999887]]
    
    迭代10000次，当前loss:1.0441840844140337e-13, 当前权重:[[3.00000002 7.0000001  4.00000003]
     [5.00000002 2.00000009 6.00000003]],当前偏置[[1.99999984 8.99999912 2.99999976]]
    
    迭代11000次，当前loss:2.3555979316041686e-16, 当前权重:[[3.         7.00000002 4.00000001]
     [5.         2.00000002 6.        ]],当前偏置[[1.99999997 8.99999983 2.99999995]]
    
    迭代11790次，当前loss:3.2760254909770205e-22, 当前权重:[[3. 7. 4.]
     [5. 2. 6.]],当前偏置[[1.99999999 8.99999995 2.99999999]]



```python
print("W1==W: {} \nb1=b:  {}".format(np.allclose(W1,W),np.allclose(np.squeeze(b1),b)))
```

    W1==W: True 
    b1=b:  True

说明回归拟合正确！